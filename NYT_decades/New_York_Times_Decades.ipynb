{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York Times Over the Decades\n",
    "## Unsupervised Learning Capstone\n",
    "The world has changed over the past decades. Has what we read about in the media changed? This project will utilize data from the [New York Times (NYT) archives API](https://developer.nytimes.com/archive_api.json).  From these archives, the NYT provides the lead paragraph from the article, along with other information about the article.  Using these lead paragraphs, I used the natural language processing techniques of Term Frequency Inverse Document Frequency Vectorization to turn paragraphs into vectors by determining the frequent and infrequent words and how frequently the words were used. I also used spaCy to determine the number of various parts of speech that were used in the paragraphs.  Using the best 5% of the features, I used various clustering methods to group the sentences based on similarity.  Finally, I used these features to predict the decade from which a sentence of a New York Times article was written.  \n",
    "\n",
    "This modeling be useful for authors to determine the patterns of writing over time, especially the changes, or lack thereof, in topics and ways to help people better understand the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Natural Language Processing imports\n",
    "import requests\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Modeling imports\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Selection and Cleaning\n",
    "For my selected texts, I decided to use the New York Times article API to get a selection of texts, all from December, every 10 years from 1960 to 2010. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "API_KEY = '5cb4f9a5273b4fbf97ef0d7d01eb6273'\n",
    "\n",
    "# Get Requests to pull JSON data\n",
    "request_2010 = requests.get('http://api.nytimes.com/svc/archive/v1/2010/12.json?api-key=5cb4f9a5273b4fbf97ef0d7d01eb6273')\n",
    "request_2000 = requests.get('http://api.nytimes.com/svc/archive/v1/2000/12.json?api-key=5cb4f9a5273b4fbf97ef0d7d01eb6273')\n",
    "request_1990 = requests.get('http://api.nytimes.com/svc/archive/v1/1990/12.json?api-key=5cb4f9a5273b4fbf97ef0d7d01eb6273')\n",
    "request_1980 = requests.get('http://api.nytimes.com/svc/archive/v1/1980/12.json?api-key=5cb4f9a5273b4fbf97ef0d7d01eb6273')\n",
    "request_1970 = requests.get('http://api.nytimes.com/svc/archive/v1/1970/12.json?api-key=5cb4f9a5273b4fbf97ef0d7d01eb6273')\n",
    "request_1960 = requests.get('http://api.nytimes.com/svc/archive/v1/1960/12.json?api-key=5cb4f9a5273b4fbf97ef0d7d01eb6273')\n",
    "# Gathering responses from JSON data\n",
    "response_2010 = request_2010.json()\n",
    "response_2000 = request_2000.json()\n",
    "response_1990 = request_1990.json()\n",
    "response_1980 = request_1980.json()\n",
    "response_1970 = request_1970.json()\n",
    "response_1960 = request_1960.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting document information from JSON\n",
    "docs_2010 = response_2010['response']['docs']\n",
    "docs_2000 = response_2000['response']['docs']\n",
    "docs_1990 = response_1990['response']['docs']\n",
    "docs_1980 = response_1980['response']['docs']\n",
    "docs_1970 = response_1970['response']['docs']\n",
    "docs_1960 = response_1960['response']['docs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now that I've gathered the texts, let's see a sampling of a lead paragraph to see what we're getting into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The best-selling novelist Brad Meltzer leads a team of investigators in exploring mysteries of American history.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[docs_2010[2]['lead_paragraph']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's extract the lead paragraph from each of the first 100 articles from each year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_2010 = []\n",
    "for article in docs_2010[0:100]:\n",
    "    art = [article['lead_paragraph']]\n",
    "    for i in range(len(art)):\n",
    "        nyt_2010.append([art[i], '2010'])\n",
    "\n",
    "nyt_2000 = []\n",
    "for article in docs_2000[0:100]:\n",
    "    art = [article['lead_paragraph']]\n",
    "    for i in range(len(art)):\n",
    "        nyt_2000.append([art[i], '2000'])\n",
    "\n",
    "nyt_1990 = []\n",
    "for article in docs_1990[0:100]:\n",
    "    art = [article['lead_paragraph']]\n",
    "    for i in range(len(art)):\n",
    "        nyt_1990.append([art[i], '1990'])\n",
    "\n",
    "nyt_1980 = []\n",
    "for article in docs_1980[0:100]:\n",
    "    art = [article['lead_paragraph']]\n",
    "    for i in range(len(art)):\n",
    "        nyt_1980.append([art[i], '1980'])\n",
    "\n",
    "nyt_1970 = []\n",
    "for article in docs_1970[0:100]:\n",
    "    art = [article['lead_paragraph']]\n",
    "    for i in range(len(art)):\n",
    "        nyt_1970.append([art[i], '1970'])\n",
    "        \n",
    "nyt_1960 = []\n",
    "for article in docs_1960[0:100]:\n",
    "    art = [article['lead_paragraph']]\n",
    "    for i in range(len(art)):\n",
    "        nyt_1960.append([art[i], '1960'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have gathered the information and labeled each with its respective year published, let's combine all of these years into one data frame to then be able to manipulate and use for analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_paragraph</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boulder’s Uptown, with its new shops and resta...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With 10 nods for his comeback album, \"Recovery...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The best-selling novelist Brad Meltzer leads a...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nicholas D. Kristof visits a Haitian cholera t...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nicholas D. Kristof reports from Haiti about t...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      lead_paragraph  year\n",
       "0  Boulder’s Uptown, with its new shops and resta...  2010\n",
       "1  With 10 nods for his comeback album, \"Recovery...  2010\n",
       "2  The best-selling novelist Brad Meltzer leads a...  2010\n",
       "3  Nicholas D. Kristof visits a Haitian cholera t...  2010\n",
       "4  Nicholas D. Kristof reports from Haiti about t...  2010"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = [nyt_2010, nyt_2000, nyt_1990, nyt_1980, nyt_1970, nyt_1960]\n",
    "nyt_all = pd.DataFrame(columns=['lead_paragraph', 'year'])\n",
    "for year in years:\n",
    "    for i in range(len(nyt_2010)):\n",
    "        nyt_all = nyt_all.append({'lead_paragraph':year[i][0], 'year':year[i][1]}, ignore_index=True)\n",
    "nyt_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in order to best analyze these lead paragaphs, let's define a function to clean out the double dashes, which will not be able to be processed by natural language processing, remove all numbers, which will not provide any decade relevant information, take the lowercase of all words for consistency, and then remove all extra white space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--', ' ', text)\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    #text = re.sub(r'\\.', '. ', text)\n",
    "    text = text.lower()\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_paragraph</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>boulder’s uptown, with its new shops and resta...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>with nods for his comeback album, \"recovery,\" ...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the best-selling novelist brad meltzer leads a...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nicholas d. kristof visits a haitian cholera t...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nicholas d. kristof reports from haiti about t...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      lead_paragraph  year\n",
       "0  boulder’s uptown, with its new shops and resta...  2010\n",
       "1  with nods for his comeback album, \"recovery,\" ...  2010\n",
       "2  the best-selling novelist brad meltzer leads a...  2010\n",
       "3  nicholas d. kristof visits a haitian cholera t...  2010\n",
       "4  nicholas d. kristof reports from haiti about t...  2010"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean all lead paragraphs\n",
    "nyt_all['lead_paragraph'] = nyt_all.lead_paragraph.map(lambda x: text_cleaner(str(x)))\n",
    "nyt_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will set aside 25% of the corpus as a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying variables\n",
    "X = nyt_all['lead_paragraph']\n",
    "y = nyt_all['year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our lead paragraphs labeled and cleaned, we are ready for further analysis of these texts.\n",
    "\n",
    "## Feature Generation \n",
    "### Tf-idf Vectorization\n",
    "To best cluster our data, we will first turn our paragraphs into vectors.  Term Frequency Inverse Document Frequency (Tf-idf) vectorization takes into account how many times a particular word appears in a document and then takes into account the infrequent words to create a vector for each individual word. \n",
    "\n",
    "For this vectorizer, we will use the following parameters:\n",
    "- Drop the words that occur in more than half the paragraphs\n",
    "- Only use words that appear at least 4 times\n",
    "- Drop English stop words\n",
    "- Leave the text in lowercase, which was already cleaned out\n",
    "- Apply a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "- Add 1 to all document frequencies to prevent divide-by-zero errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1599\n",
      "Original sentence: boulder’s uptown, with its new shops and restaurants, is worth a visit.\n",
      "Tf_idf vector: {'new': 0.26399997174868384, 'shops': 0.5922351826476437, 'worth': 0.5239574673092323, 'visit': 0.5522952795397594}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5,\n",
    "                            min_df=3,\n",
    "                            stop_words='english',\n",
    "                            lowercase=False,\n",
    "                            use_idf=True,\n",
    "                            norm=u'l2',\n",
    "                            smooth_idf=True)\n",
    "# Applying the vectorizer\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "print('Number of features: {}'.format(X_tfidf.get_shape()[1]))\n",
    "\n",
    "# Reshape vectorizer to readable content\n",
    "X_tfidf_csr = X_tfidf.tocsr()\n",
    "\n",
    "# Number of paragraphs\n",
    "n = X_tfidf_csr.shape[0]\n",
    "\n",
    "# A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "\n",
    "# List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# For each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_tfidf_csr[i, j]\n",
    "\n",
    "# Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X[0])\n",
    "print('Tf_idf vector:', tfidf_bypara[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will normalize our data to best cluster data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "X_norm = normalize(X_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing - spaCy\n",
    "Next, we will tokenize each sentence to be able to extract information about parts of speech to add as features in our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating spaCy\n",
    "nlp = spacy.load('en')\n",
    "X_words = []\n",
    "\n",
    "for row in X:\n",
    "    # Processing each row for tokens\n",
    "    row_doc = nlp(row)\n",
    "    # Calculating length of each sentence\n",
    "    sent_len = len(row_doc) \n",
    "    # Initializing counts of different parts of speech\n",
    "    advs = 0\n",
    "    verb = 0\n",
    "    noun = 0\n",
    "    adj = 0\n",
    "    for token in row_doc:\n",
    "        # Identifying each part of speech and adding to counts\n",
    "        if token.pos_ == 'ADV':\n",
    "            advs +=1\n",
    "        elif token.pos_ == 'VERB':\n",
    "            verb +=1\n",
    "        elif token.pos_ == 'NOUN':\n",
    "            noun +=1\n",
    "        elif token.pos_ == 'ADJ':\n",
    "            adj +=1\n",
    "    # Creating a list of all features for each sentence\n",
    "    X_words.append([row_doc, advs, verb, noun, adj, sent_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining our new features and our year labels for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BOW</th>\n",
       "      <th>ADV</th>\n",
       "      <th>VERB</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(boulder, ’s, uptown, ,, with, its, new, shops...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(with, nods, for, his, comeback, album, ,, \", ...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(the, best, -, selling, novelist, brad, meltze...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(nicholas, d., kristof, visits, a, haitian, ch...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(nicholas, d., kristof, reports, from, haiti, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 BOW  ADV  VERB  NOUN  ADJ  \\\n",
       "0  (boulder, ’s, uptown, ,, with, its, new, shops...    0     1     5    3   \n",
       "1  (with, nods, for, his, comeback, album, ,, \", ...    0     4    11    6   \n",
       "2  (the, best, -, selling, novelist, brad, meltze...    1     3     7    1   \n",
       "3  (nicholas, d., kristof, visits, a, haitian, ch...    0     3     4    1   \n",
       "4  (nicholas, d., kristof, reports, from, haiti, ...    1     7     9    2   \n",
       "\n",
       "   sent_length  year  \n",
       "0           16  2010  \n",
       "1           36  2010  \n",
       "2           19  2010  \n",
       "3           10  2010  \n",
       "4           28  2010  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data frame for features\n",
    "nyt_bow = pd.DataFrame(data=X_words, columns=['BOW', 'ADV', 'VERB', 'NOUN', 'ADJ', 'sent_length'])\n",
    "# Adding in year data\n",
    "nyt_bow = pd.concat([nyt_bow, y], ignore_index=False, axis=1)\n",
    "nyt_bow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will combine our Tf-idf vectors and our newly created features into one data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BOW</th>\n",
       "      <th>ADV</th>\n",
       "      <th>VERB</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>year</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>...</th>\n",
       "      <th>1589</th>\n",
       "      <th>1590</th>\n",
       "      <th>1591</th>\n",
       "      <th>1592</th>\n",
       "      <th>1593</th>\n",
       "      <th>1594</th>\n",
       "      <th>1595</th>\n",
       "      <th>1596</th>\n",
       "      <th>1597</th>\n",
       "      <th>1598</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(boulder, ’s, uptown, ,, with, its, new, shops...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(with, nods, for, his, comeback, album, ,, \", ...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(the, best, -, selling, novelist, brad, meltze...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(nicholas, d., kristof, visits, a, haitian, ch...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(nicholas, d., kristof, reports, from, haiti, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1606 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 BOW  ADV  VERB  NOUN  ADJ  \\\n",
       "0  (boulder, ’s, uptown, ,, with, its, new, shops...    0     1     5    3   \n",
       "1  (with, nods, for, his, comeback, album, ,, \", ...    0     4    11    6   \n",
       "2  (the, best, -, selling, novelist, brad, meltze...    1     3     7    1   \n",
       "3  (nicholas, d., kristof, visits, a, haitian, ch...    0     3     4    1   \n",
       "4  (nicholas, d., kristof, reports, from, haiti, ...    1     7     9    2   \n",
       "\n",
       "   sent_length  year    0    1    2  ...   1589  1590  1591  1592  1593  1594  \\\n",
       "0           16  2010  0.0  0.0  0.0  ...    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "1           36  2010  0.0  0.0  0.0  ...    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "2           19  2010  0.0  0.0  0.0  ...    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "3           10  2010  0.0  0.0  0.0  ...    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "4           28  2010  0.0  0.0  0.0  ...    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   1595  1596  1597  1598  \n",
       "0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 1606 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm_df = pd.DataFrame(data=X_norm.toarray())\n",
    "nyt_tfidf_bow = pd.concat([nyt_bow, X_norm_df], ignore_index=False, axis=1)\n",
    "nyt_tfidf_bow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "To select the best features from these words, I will use the select k best to select the 100 best features based on chi-squared values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying features and labels to choose from\n",
    "features = nyt_tfidf_bow.drop(['BOW', 'year'], axis=1)\n",
    "y = nyt_tfidf_bow.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Instantiating and fitting the 150 best features\n",
    "kpercentile = SelectPercentile(chi2, percentile=5)\n",
    "X2 = kpercentile.fit_transform(features, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Clustering\n",
    "We will first try to create a series of clusters to group the paragraphs to see if the clusters group according to decades or other themes.  We will explore a couple of different clustering methods to see which best models the data.  Before doing so, we will split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, X2_test, y_train, y_test = train_test_split(X2, y, test_size=0.25, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "We will start with K-Means clustering, where each point will be clustered based on minimizing the inertia, or sum of squared differences between the mean of the cluster and the data points of the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0   0   1  2   3  4   5\n",
       "year                       \n",
       "1960   27   0  0   2  0  47\n",
       "1970   54   0  0   1  0  21\n",
       "1980   30   6  1  23  0  10\n",
       "1990   11  19  1  36  4   2\n",
       "2000   19  16  0  37  1   3\n",
       "2010   49   1  0   9  0  20"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Calulate predicted values\n",
    "kmeans = KMeans(n_clusters=6, init='k-means++', random_state=42, n_init=20)\n",
    "y_pred = kmeans.fit_predict(X2_train)\n",
    "\n",
    "pd.crosstab(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: 0.1155519\n",
      "Silhouette Score: 0.589875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y_train, y_pred)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(X2_train, y_pred, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the adjusted rand score, this model is better than random at assigning the correct year to the sentence, and according to the silhouette score, this model pretty good at assigning paragraphs into clusters.  These clusters might not be based on decades, but maybe based on topic or author.  Let's try another model.\n",
    "\n",
    "### Mini Batch K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0   0  1   2   3  4   5  6\n",
       "year                          \n",
       "1960    2  0  27   0  0  47  0\n",
       "1970    2  0  53   0  0  21  0\n",
       "1980   22  0  29   9  0   9  1\n",
       "1990   36  1  10  20  5   1  0\n",
       "2000   34  0  16  22  1   3  0\n",
       "2010   11  0  49   1  0  18  0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "minikmeans = MiniBatchKMeans(n_clusters=7, init='k-means++', random_state=42, init_size=1000, batch_size=1000)\n",
    "y_pred2 = minikmeans.fit_predict(X2_train)\n",
    "\n",
    "pd.crosstab(y_train, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: 0.120797\n",
      "Silhouette Score: 0.5888146\n"
     ]
    }
   ],
   "source": [
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y_train, y_pred2)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(X2_train, y_pred2, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this model is somewhat okay at identifying sentences from decades, but worse than k-means.  And the model is pretty good at assigning sentences to tight clusters, maybe not based on year, but perhaps topic.  Let's try again.\n",
    "\n",
    "### Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/manifold/spectral_embedding_.py:234: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\"Graph is not fully connected, spectral embedding\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>43</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>21</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>8</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>2</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0   0   1  2  3  4  5\n",
       "year                     \n",
       "1960   43  23  2  6  0  2\n",
       "1970   21  45  3  7  0  0\n",
       "1980    8  55  2  5  0  0\n",
       "1990    1  67  0  3  2  0\n",
       "2000    2  72  0  2  0  0\n",
       "2010    1  65  3  8  0  2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "n_clusters= 6\n",
    "sc = SpectralClustering(n_clusters=n_clusters)\n",
    "y_pred3 = sc.fit_predict(X2_train)\n",
    "\n",
    "pd.crosstab(y_train, y_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: 0.05318995\n",
      "Silhouette Score: -0.313308\n"
     ]
    }
   ],
   "source": [
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y_train, y_pred3)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(X2_train, y_pred3, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the adjusted rand score and silhouette score, this model no different than the model choosing random cluster assignments.  Let's try a final model.\n",
    "\n",
    "### Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of estimated clusters: 13\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  0   1   2   3   4   5   6   7   8   9   10  11  12\n",
       "year                                                     \n",
       "1960    0   0   0   2   0  23   0   4  47   0   0   0   0\n",
       "1970    0   0   0   2   0  33   0  20  21   0   0   0   0\n",
       "1980    0   6   1   6   3  18   0  11   9   1   4   0  11\n",
       "1990    3  21   5   9   7   4   1   6   1   0   8   2   6\n",
       "2000    0  10   7  18   6   5   0  11   3   0   6   1   9\n",
       "2010    0   6   1   4   0  33   0  17  18   0   0   0   0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "af = AffinityPropagation(damping=0.85, max_iter=550, copy=False)\n",
    "y_pred4 = af.fit_predict(X2_train)\n",
    "\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "n_clusters = len(cluster_centers_indices)\n",
    "print('Number of estimated clusters: {}'.format(n_clusters))\n",
    "\n",
    "pd.crosstab(y_train, y_pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: 0.1055845\n",
      "Silhouette Score: 0.5018931\n"
     ]
    }
   ],
   "source": [
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y_train, y_pred4)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(X2_train, y_pred4, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of these models are particularly good at identifying the decade from which the sentence was written. This might be because the same types of words are used in each decade, and the specific events weren't covered enough in the data set to be able to distinguish the difference.  The models were able to pretty well separate sentences based on similarities. \n",
    "\n",
    "The K-Means clustering model is best at identifying the year and clustering the sentences.  I will add this as a feature to our training set to then use for supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train_c = pd.DataFrame(X2_train)\n",
    "X2_train_c['kmeans_clust'] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models \n",
    "Now, I will use supervised classification models with and without the k-means clustering predictions as features to see how well our model performs.  I will start with the default settings in the models to get a baseline score.\n",
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score with clustering: 0.40478(+/- 0.056)\n",
      "\n",
      "Training set score without clustering:0.39796(+/- 0.047)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rfc_c = RandomForestClassifier()\n",
    "train_c = rfc_c.fit(X2_train_c, y_train)\n",
    "rfc_c_scores = cross_val_score(rfc_c, X2_train_c, y_train, cv=5)\n",
    "print('Training set score with clustering: {:.5f}(+/- {:.3f})'.format(rfc_c_scores.mean(), rfc_c_scores.std()*2))\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "train = rfc.fit(X2_train, y_train)\n",
    "rfc_scores = cross_val_score(rfc, X2_train, y_train, cv=5)\n",
    "print('\\nTraining set score without clustering:{:.5f}(+/- {:.3f})'.format(rfc_scores.mean(), rfc_scores.std()*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest classifier performed better with the clustering prediction feature. This could be because of its poor clustering according to our ground state, but still adding something to our model. Let's see about another classification model.\n",
    "\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score with clustering: 0.39351(+/- 0.060)\n",
      "\n",
      "Training set score without clustering:0.39842(+/- 0.084)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_c = LogisticRegression()\n",
    "train_c = lr_c.fit(X2_train_c, y_train)\n",
    "lr_c_scores = cross_val_score(lr_c, X2_train_c, y_train, cv=5)\n",
    "print('Training set score with clustering: {:.5f}(+/- {:.3f})'.format(lr_c_scores.mean(), lr_c_scores.std()*2))\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X2_train, y_train)\n",
    "lr_scores = cross_val_score(lr, X2_train, y_train, cv=5)\n",
    "print('\\nTraining set score without clustering:{:.5f}(+/- {:.3f})'.format(lr_scores.mean(), lr_scores.std()*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model performed better without clustering than with it. This model also had a lower average accuracy score than the random forest classifier.  We will try one last model before making a decision.\n",
    "\n",
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score with clustering: 0.49172(+/- 0.111)\n",
      "\n",
      "Training set score without clustering:0.48072(+/- 0.083)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf_c = GradientBoostingClassifier()\n",
    "train_c = clf_c.fit(X2_train_c, y_train)\n",
    "clf_c_scores = cross_val_score(clf_c, X2_train_c, y_train, cv=5)\n",
    "print('Training set score with clustering: {:.5f}(+/- {:.3f})'.format(clf_c_scores.mean(), clf_c_scores.std()*2))\n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "train = clf.fit(X2_train, y_train)\n",
    "clf_scores = cross_val_score(clf, X2_train, y_train, cv=5)\n",
    "print('\\nTraining set score without clustering:{:.5f}(+/- {:.3f})'.format(clf_scores.mean(), clf_scores.std()*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient boosting classifier is marginally better with the clustering than without the clustering.  This higher accuracy is also higher than any of the other two models, and also has a slightly higher variance between folds than the random forest. I will now attempt to optimize the parameters of this gradient boosting model to improve the accuracy of this model.\n",
    "\n",
    "### Optimizing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.54\n",
      "Best Parameters: {'loss': 'deviance', 'max_depth': 6, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 100, 'n_estimators': 50, 'subsample': 0.6}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set of parameters to test for best score in Grid Search CV\n",
    "parameters = {'loss':['deviance'],\n",
    "               'min_samples_split':[50,100,200],\n",
    "             'min_samples_leaf':[1,2,4],\n",
    "             'max_depth':[3,4,5,6],\n",
    "             'max_features':['sqrt'],\n",
    "             'subsample':[0.6,0.8],\n",
    "             'n_estimators':[50,100,150]}\n",
    "\n",
    "#fitting model and printing best parameters and score from model\n",
    "grid_clf_c = GridSearchCV(clf_c, param_grid=parameters)\n",
    "grid_clf_c.fit(X2_train_c, y_train)\n",
    "\n",
    "print('Best Score:', grid_clf_c.best_score_)\n",
    "best_params_clf_c = grid_clf_c.best_params_\n",
    "print('Best Parameters:', best_params_clf_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized training set score with clustering:0.52027(+/- 0.047)\n"
     ]
    }
   ],
   "source": [
    "clf_c2 = GradientBoostingClassifier(**best_params_clf_c)\n",
    "train = clf_c2.fit(X2_train_c, y_train)\n",
    "clf_c2_scores = cross_val_score(clf_c2, X2_train_c, y_train, cv=5)\n",
    "print('Optimized training set score with clustering:{:.5f}(+/- {:.3f})'.format(clf_c2_scores.mean(), clf_c2_scores.std()*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Processing and Modeling\n",
    "\n",
    "Now that we've tried out what works best with the training set, I will process the test set in an identical manner as the training set in order to determine how well this model predicts the decade from which the sentence came on a new set of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0   0  1  2   3  4   5\n",
       "year                      \n",
       "1960   12  0  0   0  0  12\n",
       "1970   12  0  0   1  0  11\n",
       "1980   15  4  0   8  0   3\n",
       "1990    3  6  1  14  2   1\n",
       "2000    8  3  0  11  1   1\n",
       "2010   10  0  0   5  0   6"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calulate predicted values\n",
    "y_pred_test = kmeans.predict(X2_test)\n",
    "\n",
    "pd.crosstab(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: 0.07415039\n",
      "Silhouette Score: 0.5402269\n"
     ]
    }
   ],
   "source": [
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y_test, y_pred_test)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(X2_test, y_pred_test, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not terrible.  It looks like there was a 0.05 drop in both the adjusted rand score and the silhouette score from the training set to the test set.  This doesn't look like it's overfitting too much. According to the K-Means training set cross tabulation, the majority of sentences were clustered into clusters 0, 2, and 4, and the test set appears that it also has the majority of sentences labeled as 0, or 2, and some 4.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_test_c = pd.DataFrame(X2_test)\n",
    "X2_test_c['kmeans_clust'] = y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.20065(+/- 0.023)\n"
     ]
    }
   ],
   "source": [
    "clf_c2_scores_test = cross_val_score(clf_c2, X2_test_c, y_test, cv=5)\n",
    "print('Test set score: {:.5f}(+/- {:.3f})'.format(clf_c2_scores_test.mean(), clf_c2_scores_test.std()*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is incredibly overfit - the accuracy of the training set was around twice of that of the test set.  This means that the clusters and the models are not well able to distinguish between decades when given a piece of novel text.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Natural language processing of texts from various recent decades are difficult to cluster and classify.  This is likely because the language we have used to explain events hasn't changed much, but some of the events have.  With our quick news cycle there isn't enough continued coverage of any certain event to really characterize the decade and aid in clustering and classification.  \n",
    "\n",
    "With clustering of texts in particular, it might be better to cluster based on topic or author, as these have more distinct words or types of words that are used to describe the events or author. As in our model, then this cluster prediction could be used to support the classification models, even if they do not perfectly match the decade in which the piece was written. Interesting to note, the clustering models were improved upon adding in features to describe the number of different parts of speech in each sentence of the text. This is likely because different events in the decades require different parts of speech to accurately depict. This poor clustering also could be due to the lack of evolution of topics covered by the media in that the public is still interested in the same types of articles today as we were in the 1960s. \n",
    "\n",
    "The classificaiton of texts from different decades was also challenging even using optimized gradient boosting classification.  With each sentence carrying such different information, it was challenging to predict the decade from which the document was written.  This is evidenced by the accuracy of the test set that was about half of the accuracy of the optimized model.  Classification is usually best used when the features are highly correlated to the different classes and more consistent amongst the data, whereas with text data this isn't necessarily the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
